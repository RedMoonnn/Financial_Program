import json
import os

from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

# ä»é…ç½®ä¸­è·å–DeepSeekå‚æ•°è®¾ç½®
try:
    from core.config import deepseek_settings

    DEFAULT_MAX_TOKENS = deepseek_settings.max_tokens
    DEFAULT_TEMPERATURE = deepseek_settings.temperature
    DEFAULT_TOP_P = deepseek_settings.top_p
    DEFAULT_FREQUENCY_PENALTY = deepseek_settings.frequency_penalty
    DEFAULT_PRESENCE_PENALTY = deepseek_settings.presence_penalty
except ImportError:
    # å¦‚æœé…ç½®æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
    DEFAULT_MAX_TOKENS = 8192
    DEFAULT_TEMPERATURE = 0.7
    DEFAULT_TOP_P = 0.95
    DEFAULT_FREQUENCY_PENALTY = 0.0
    DEFAULT_PRESENCE_PENALTY = 0.0


class DeepseekAgent:
    @staticmethod
    def clean_history(history, max_items=5):
        """
        æ¸…ç†å†å²å¯¹è¯ï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ æ¡æœ‰æ•ˆå¯¹è¯
        """
        if not history:
            return None

        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºåˆ—è¡¨
        if isinstance(history, str):
            try:
                history = json.loads(history)
            except (json.JSONDecodeError, ValueError):
                return None

        # è¿‡æ»¤æ‰æ— æ•ˆå¯¹è¯
        valid_history = []
        for i, item in enumerate(history):
            if isinstance(item, dict):
                question = item.get("question", "").strip()
                # ä¿ç•™æœ€åä¸€æ¡æ¶ˆæ¯ï¼Œå³ä½¿æ˜¯ç®€å•çš„é—®å€™
                is_last = i == len(history) - 1

                # è¿‡æ»¤æ¡ä»¶
                if len(question) > 1 or is_last:
                    valid_history.append(item)

        # åªä¿ç•™æœ€è¿‘çš„å‡ æ¡å¯¹è¯
        return valid_history[-max_items:] if valid_history else None

    @staticmethod
    def build_prompt(flow_data, user_message, history=None, style="ä¸“ä¸š"):
        """
        ä¼˜åŒ–çš„promptæ„å»ºï¼šä¼˜å…ˆå›ç­”ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œç„¶åç»“åˆèµ„é‡‘æµæ•°æ®ç»™å‡ºåˆ†æ
        """
        # æ¸…ç†å†å²å¯¹è¯
        cleaned_history = DeepseekAgent.clean_history(history)

        # æ„å»ºæ•°æ®éƒ¨åˆ† - ä»…åœ¨æœ‰æ•°æ®æ—¶æ·»åŠ 
        data_section = ""
        if flow_data:
            data_str = json.dumps(flow_data, ensure_ascii=False, indent=2)
            data_section = f"""
### ğŸ“Š ç›¸å…³èµ„é‡‘æµæ•°æ®
ä»¥ä¸‹æ•°æ®ä»…ä½œä¸ºå›ç­”çš„å‚è€ƒä¾æ®ï¼Œè¯·æ ¹æ®ç”¨æˆ·é—®é¢˜åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨ï¼š
```json
{data_str}
```
"""

        # ä¼˜åŒ–åçš„ prompt ç»“æ„ï¼šé‡‡ç”¨ç»“æ„åŒ–æç¤ºè¯
        prompt = f"""
### ğŸ¯ ç”¨æˆ·æ ¸å¿ƒé—®é¢˜
{user_message}

{data_section}

### ğŸ“ å›ç­”åŸåˆ™
1. **ä¼˜å…ˆå“åº”é—®é¢˜**ï¼šç›´æ¥é’ˆå¯¹ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜è¿›è¡Œå›ç­”ï¼Œä¸è¦é¡¾å·¦å³è€Œè¨€ä»–ã€‚
2. **æ•°æ®é©±åŠ¨åˆ†æ**ï¼š
   - å¦‚æœç”¨æˆ·é—®é¢˜æ¶‰åŠå…·ä½“çš„è‚¡ç¥¨/æ¿å—ï¼Œä¸”ä¸Šæ–¹ã€ç›¸å…³èµ„é‡‘æµæ•°æ®ã€‘ä¸­æœ‰å¯¹åº”æ•°æ®ï¼Œè¯·åŠ¡å¿…ç»“åˆæ•°æ®ï¼ˆå¦‚ä¸»åŠ›å‡€æµå…¥ã€è¶…å¤§å•å æ¯”ç­‰ï¼‰è¿›è¡Œé‡åŒ–åˆ†æã€‚
   - å¦‚æœæ•°æ®ä¸é—®é¢˜æ— å…³ï¼ˆä¾‹å¦‚ç”¨æˆ·é—®"ä»€ä¹ˆæ˜¯è‚¡ç¥¨"ï¼‰ï¼Œè¯·å¿½ç•¥æ•°æ®ï¼Œä»…åˆ©ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å›ç­”ã€‚
3. **è¾“å‡ºé£æ ¼**ï¼š
   - è¯·ä½¿ç”¨**{style}**é£æ ¼ã€‚
   - è¯­è¨€ç®€ç»ƒï¼Œé€»è¾‘æ¸…æ™°ï¼Œå…³é”®ç»“è®ºå¯ä»¥åŠ ç²—ã€‚
   - é¿å…å †ç Œè¿‡äºæ™¦æ¶©çš„æœ¯è¯­ï¼Œå¿…è¦æ—¶è¿›è¡Œè§£é‡Šã€‚
"""

        # æ·»åŠ å†å²å¯¹è¯ä¸Šä¸‹æ–‡
        if cleaned_history:
            # åªä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå‡å°‘tokenæ¶ˆè€—
            history_summary = []
            for item in cleaned_history:
                q = (
                    item.get("question", "")[:100] + "..."
                    if len(item.get("question", "")) > 100
                    else item.get("question", "")
                )
                a = item.get("answer", "")
                # å¤„ç† answer å¯èƒ½æ˜¯ dict çš„æƒ…å†µ
                if isinstance(a, dict):
                    advice = a.get("text") or a.get("advice") or str(a)
                    a_text = str(advice)[:200]
                else:
                    a_text = str(a)[:200]

                history_summary.append(f"User: {q}\nAssistant: {a_text}...")

            prompt += "\n### ğŸ•’ æœ€è¿‘å¯¹è¯ä¸Šä¸‹æ–‡\n" + "\n".join(history_summary)

        # æ£€æŸ¥prompté•¿åº¦é™åˆ¶ï¼ˆåŸºäºtokenä¼°ç®—ï¼Œ1 token â‰ˆ 4å­—ç¬¦ï¼Œ64k tokens â‰ˆ 256kå­—ç¬¦ï¼‰
        # ä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œè®¾ç½®ä¸€ä¸ªåˆç†çš„å­—ç¬¦é™åˆ¶ï¼ˆçº¦50kå­—ç¬¦ï¼Œå¯¹åº”çº¦12.5k tokensçš„è¾“å…¥ï¼‰
        max_prompt_chars = 50000
        if len(prompt) > max_prompt_chars:
            print(
                f"Warning: Prompt too long ({len(prompt)} chars), truncating to {max_prompt_chars} chars...",
                flush=True,
            )
            prompt = prompt[:max_prompt_chars] + "\n\n[æç¤ºï¼šç”±äºä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œéƒ¨åˆ†å†…å®¹å·²æˆªæ–­]"
        return prompt

    @staticmethod
    def chat(
        user_message,
        system_message=None,
        stream=False,
        max_tokens=None,
        temperature=None,
        top_p=None,
        frequency_penalty=None,
        presence_penalty=None,
    ):
        """
        ä½¿ç”¨ deepseek-chat æ¨¡å‹è¿›è¡Œå¿«é€Ÿå¯¹è¯ï¼ˆéæ¨ç†æ¨¡å‹ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰
        é€‚ç”¨äºæŠ¥å‘Šç”Ÿæˆç­‰ä¸éœ€è¦æ¨ç†è¿‡ç¨‹çš„åœºæ™¯

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            system_message: ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆé»˜è®¤8192ï¼Œæœ€å¤§æ”¯æŒ8192ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼Œé»˜è®¤0.7ï¼‰
            top_p: æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶é‡‡æ ·çš„å¤šæ ·æ€§ï¼ˆ0-1ï¼Œé»˜è®¤0.95ï¼‰
            frequency_penalty: é¢‘ç‡æƒ©ç½šï¼Œå‡å°‘é‡å¤å†…å®¹ï¼ˆ-2åˆ°2ï¼Œé»˜è®¤0.0ï¼‰
            presence_penalty: å­˜åœ¨æƒ©ç½šï¼Œé¼“åŠ±æ–°è¯é¢˜ï¼ˆ-2åˆ°2ï¼Œé»˜è®¤0.0ï¼‰

        Returns:
            å¦‚æœ stream=False: è¿”å›å®Œæ•´æ–‡æœ¬å­—ç¬¦ä¸²
            å¦‚æœ stream=True: è¿”å›ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡yieldæ–‡æœ¬å†…å®¹
        """
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)

        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": user_message})

        # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å‚æ•°
        if max_tokens is None:
            max_tokens = DEFAULT_MAX_TOKENS
        if temperature is None:
            temperature = DEFAULT_TEMPERATURE
        if top_p is None:
            top_p = DEFAULT_TOP_P
        if frequency_penalty is None:
            frequency_penalty = DEFAULT_FREQUENCY_PENALTY
        if presence_penalty is None:
            presence_penalty = DEFAULT_PRESENCE_PENALTY

        request_payload = {
            "model": "deepseek-chat",
            "messages": messages,
            "stream": stream,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty,
        }

        try:
            if stream:
                # å®šä¹‰å†…éƒ¨ç”Ÿæˆå™¨å‡½æ•°ç”¨äºæµå¼è¾“å‡º
                def stream_generator():
                    try:
                        response = client.chat.completions.create(**request_payload)
                        for chunk in response:
                            if not chunk.choices or len(chunk.choices) == 0:
                                continue
                            delta = chunk.choices[0].delta
                            if delta and hasattr(delta, "content") and delta.content:
                                yield delta.content
                    except Exception as e:
                        import traceback

                        error_detail = traceback.format_exc()
                        print(f"Chat stream error: {e}\n{error_detail}", flush=True)
                        yield f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"

                return stream_generator()
            else:
                # éæµå¼è¾“å‡ºï¼Œç›´æ¥è¿”å›å®Œæ•´ç»“æœ
                response = client.chat.completions.create(**request_payload)
                return response.choices[0].message.content
        except Exception as e:
            import traceback

            error_detail = traceback.format_exc()
            print(f"Chat error: {e}\n{error_detail}", flush=True)
            if stream:

                def error_gen(err=e):
                    yield f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(err)}"

                return error_gen()
            else:
                return f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"

    @staticmethod
    def analyze(
        flow_data,
        user_message=None,
        history=None,
        style="ä¸“ä¸š",
        max_tokens=None,
        temperature=None,
        top_p=None,
        frequency_penalty=None,
        presence_penalty=None,
    ):
        """
        éæµå¼åˆ†æï¼Œç›´æ¥è¿”å›å®Œæ•´ç»“æœ
        ä½¿ç”¨ deepseek-reasoner æ¨¡å‹ï¼ˆæ¨ç†æ¨¡å‹ï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†æ›´å‡†ç¡®ï¼‰

        Args:
            flow_data: èµ„é‡‘æµæ•°æ®
            user_message: ç”¨æˆ·æ¶ˆæ¯
            history: å†å²å¯¹è¯è®°å½•
            style: è¾“å‡ºé£æ ¼
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆé»˜è®¤8192ï¼Œæœ€å¤§æ”¯æŒ8192ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼Œé»˜è®¤0.7ï¼‰
            top_p: æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶é‡‡æ ·çš„å¤šæ ·æ€§ï¼ˆ0-1ï¼Œé»˜è®¤0.95ï¼‰
            frequency_penalty: é¢‘ç‡æƒ©ç½šï¼Œå‡å°‘é‡å¤å†…å®¹ï¼ˆ-2åˆ°2ï¼Œé»˜è®¤0.0ï¼‰
            presence_penalty: å­˜åœ¨æƒ©ç½šï¼Œé¼“åŠ±æ–°è¯é¢˜ï¼ˆ-2åˆ°2ï¼Œé»˜è®¤0.0ï¼‰
        """
        full_text = ""
        full_thinking = ""

        # å¤ç”¨ analyze_stream è·å–ç»“æœ
        try:
            stream = DeepseekAgent.analyze_stream(
                flow_data,
                user_message,
                history,
                style,
                max_tokens,
                temperature,
                top_p,
                frequency_penalty,
                presence_penalty,
            )

            for chunk in stream:
                if chunk["type"] == "text":
                    full_text += chunk["content"]
                elif chunk["type"] == "thinking":
                    full_thinking += chunk["content"]
                elif chunk["type"] == "error":
                    return {"advice": f"AIåˆ†æå‡ºé”™: {chunk['content']}", "thinking": full_thinking}

            return {"advice": full_text, "thinking": full_thinking}

        except Exception as e:
            return {"advice": f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}", "thinking": ""}

    @staticmethod
    def analyze_stream(
        flow_data,
        user_message=None,
        history=None,
        style="ä¸“ä¸š",
        max_tokens=None,
        temperature=None,
        top_p=None,
        frequency_penalty=None,
        presence_penalty=None,
    ):
        """
        æµå¼åˆ†æï¼Œæ”¯æŒåŒºåˆ† Thinking å’Œ text
        è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡ yield ä¸€ä¸ªåŒ…å« type å’Œ content çš„å­—å…¸
        type å¯ä»¥æ˜¯ 'thinking' æˆ– 'text'

        Args:
            flow_data: èµ„é‡‘æµæ•°æ®
            user_message: ç”¨æˆ·æ¶ˆæ¯
            history: å†å²å¯¹è¯è®°å½•
            style: è¾“å‡ºé£æ ¼
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆé»˜è®¤8192ï¼Œæœ€å¤§æ”¯æŒ8192ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼Œé»˜è®¤0.7ï¼‰
            top_p: æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶é‡‡æ ·çš„å¤šæ ·æ€§ï¼ˆ0-1ï¼Œé»˜è®¤0.95ï¼‰
            frequency_penalty: é¢‘ç‡æƒ©ç½šï¼Œå‡å°‘é‡å¤å†…å®¹ï¼ˆ-2åˆ°2ï¼Œé»˜è®¤0.0ï¼‰
            presence_penalty: å­˜åœ¨æƒ©ç½šï¼Œé¼“åŠ±æ–°è¯é¢˜ï¼ˆ-2åˆ°2ï¼Œé»˜è®¤0.0ï¼‰
        """
        prompt = DeepseekAgent.build_prompt(flow_data, user_message, history, style)

        # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å‚æ•°
        if max_tokens is None:
            max_tokens = DEFAULT_MAX_TOKENS
        if temperature is None:
            temperature = DEFAULT_TEMPERATURE
        if top_p is None:
            top_p = DEFAULT_TOP_P
        if frequency_penalty is None:
            frequency_penalty = DEFAULT_FREQUENCY_PENALTY
        if presence_penalty is None:
            presence_penalty = DEFAULT_PRESENCE_PENALTY

        request_payload = {
            "model": "deepseek-reasoner",
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€åä¸“ä¸šé‡‘èåˆ†æå¸ˆï¼Œå–„äºèµ„é‡‘æµåˆ†æå’ŒæŠ•èµ„å»ºè®®ã€‚è¯·ä¼˜å…ˆç›´æ¥å›ç­”ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œç„¶åç»“åˆæ•°æ®ç»™å‡ºè¯¦ç»†åˆ†æã€‚",
                },
                {"role": "user", "content": prompt},
            ],
            "stream": True,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty,
        }

        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)

        try:
            stream = client.chat.completions.create(**request_payload)

            for chunk in stream:
                if not chunk.choices or len(chunk.choices) == 0:
                    continue

                delta = chunk.choices[0].delta
                if not delta:
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰ thinking å†…å®¹ï¼ˆDeepseek å¯èƒ½åœ¨ä¸åŒå­—æ®µä¸­ï¼‰
                # DeepSeek API è¿”å› reasoning_content
                thinking_content = None
                if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                    thinking_content = delta.reasoning_content
                elif hasattr(delta, "thinking") and delta.thinking:
                    thinking_content = delta.thinking
                elif hasattr(delta, "reasoning") and delta.reasoning:
                    thinking_content = delta.reasoning

                if thinking_content:
                    yield {"type": "thinking", "content": thinking_content}

                # æ£€æŸ¥æ˜¯å¦æœ‰ text å†…å®¹
                text_content = None
                if hasattr(delta, "content"):
                    text_content = getattr(delta, "content", None)
                elif isinstance(delta, dict) and "content" in delta:
                    text_content = delta.get("content")

                if text_content:
                    yield {"type": "text", "content": text_content}

        except Exception as e:
            import traceback

            error_detail = traceback.format_exc()
            print(f"Stream error: {e}\n{error_detail}", flush=True)
            yield {"type": "error", "content": f"æµå¼è¾“å‡ºé”™è¯¯: {str(e)}"}


if __name__ == "__main__":
    # æ„é€ æµ‹è¯•æ•°æ®
    flow_data = [
        {
            "type": "stock",
            "flow_type": "Stock_Flow",
            "market_type": "All_Stocks",
            "period": "today",
            "data": {
                "code": "600000",
                "name": "æµ¦å‘é“¶è¡Œ",
                "latest_price": 10.5,
                "change_percentage": 1.2,
                "main_flow_net_amount": 1000000,
                "main_flow_net_percentage": 5.6,
                "extra_large_order_flow_net_amount": 500000,
                "extra_large_order_flow_net_percentage": 2.8,
                "large_order_flow_net_amount": 200000,
                "large_order_flow_net_percentage": 1.1,
                "medium_order_flow_net_amount": 150000,
                "medium_order_flow_net_percentage": 0.8,
                "small_order_flow_net_amount": 150000,
                "small_order_flow_net_percentage": 0.9,
                "crawl_time": "2024-05-01 10:00:00",
            },
        }
    ]
    user_message = "è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹æµ¦å‘é“¶è¡Œä»Šæ—¥çš„èµ„é‡‘æµæƒ…å†µ"
    print("\n=== æœ¬åœ°æµ‹è¯•AIå¯¹è¯ ===\n")
    print("è¯·ä½¿ç”¨ analyze_stream è¿›è¡Œæµå¼åˆ†ææµ‹è¯•")
